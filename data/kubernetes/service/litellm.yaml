---
ingress_class: contour-compute
service_name: llm

values:
  ingress:
    enabled: true
    annotations:
      projectcontour.io/response-timeout: infinity
    className: "%{lookup('ingress_class')}"
    hosts:
      - host: "%{nest::kubernetes::fqdn}"
        paths:
          - path: /
            pathType: ImplementationSpecific

  proxy_config:
    model_list:
      # General reasoning
      - model_name: gpt-oss-120b
        litellm_params:
          model: openai/general
          api_base: http://llama-general/v1
          api_key: none
          max_tokens: 4096
          temperature: 0.7

      # Coding
      - model_name: qwen3-coder-30b
        litellm_params:
          model: openai/coder
          api_base: http://llama-coder/v1
          api_key: none
          max_tokens: 2048
          temperature: 0.2

      # Multimodal assistant
      - model_name: gemma-3-4b
        litellm_params:
          model: openai/multi
          api_base: http://llama-multi/v1
          api_key: none
          max_tokens: 2048
          temperature: 0.4
        model_info:
          supports_vision: True

      # Utility pool (Qwen2.5 3B x2)
      - model_name: qwen25-util
        litellm_params:
          model: openai/util
          api_base: http://llama-util:8081/v1
          api_key: none
          temperature: 0
          top_p: 0.2
      - model_name: qwen25-util
        litellm_params:
          model: openai/util
          api_base: http://llama-util:8082/v1
          api_key: none
          temperature: 0
          top_p: 0.2

    router_settings:
      routing_strategy: least-busy
